{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a39f0f-f7ba-4def-82ce-27c1c38a077a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/jeongmieun/.pyenv/versions/3.11.9/envs/note/lib/python3.11/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/jeongmieun/.pyenv/versions/3.11.9/envs/note/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1492e65-dfa1-472c-9b95-0ffdf0823f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hostname 경고 해결\n",
    "# WARN Utils: Your hostname, ... resolves to a loopback address: 127.0.0.1; using 192.168.100.101 instead\n",
    "\n",
    "import os\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"192.168.100.101\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19829cd7-7092-444e-a105-909937469e4d",
   "metadata": {},
   "source": [
    "## createDataFrame\n",
    "```\n",
    "SparkSession.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f13526c4-37c8-40cf-80d0-095d452700b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 16:28:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             country|count|\n",
      "+--------------------+-----+\n",
      "|           Australia|   10|\n",
      "|           Singapore|    9|\n",
      "|             Ecuador|    9|\n",
      "|            Dominica|    9|\n",
      "|          Madagascar|    9|\n",
      "|           Nicaragua|    9|\n",
      "|              Kuwait|    9|\n",
      "|               Congo|    9|\n",
      "|            Thailand|    9|\n",
      "|             Senegal|    8|\n",
      "|Sao Tome and Prin...|    8|\n",
      "|Virgin Islands, B...|    8|\n",
      "|              Zambia|    8|\n",
      "|  Dominican Republic|    8|\n",
      "|                Mali|    8|\n",
      "|             Belgium|    7|\n",
      "|Palestinian Terri...|    7|\n",
      "|             Lesotho|    7|\n",
      "|         Isle of Man|    7|\n",
      "|             Bolivia|    7|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import (Row, SparkSession)\n",
    "from pyspark.sql.functions import col, asc, desc\n",
    "\n",
    "def parse_line(line:str):\n",
    "    fields = line.split(\"|\")\n",
    "    return Row(\n",
    "        name = str(fields[0]),\n",
    "        country = str(fields[1]),\n",
    "        email = str(fields[2]),\n",
    "        compensation = int(fields[3])\n",
    "    )\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "# map사용을 위해 sparkContext.textFile을 사용하여 RDD로 반환\n",
    "lines = spark.sparkContext.textFile(\"file:////Users/jeongmieun/study/spark/sample/income.txt\")\n",
    "# map: RDD의 각 요소마다 주어진 함수를 적용\n",
    "income_date = lines.map(parse_line)\n",
    "\n",
    "# SparkSession.createDateFrame(data, schema=None, samplingRatio=None, verifySchema=True)\n",
    "schema_income = spark.createDataFrame(data=income_date).cache() # cache에 저장\n",
    "\n",
    "# Creates or replaces a local temporary view with this DataFrame\n",
    "schema_income.createOrReplaceTempView(\"income\")\n",
    "# schema_income.show()\n",
    "\n",
    "# avg_income = spark.sql(\n",
    "    # \"SELECT * FROM income WHERE compensation >= (SELECT avg(compensation) FROM income)\")\n",
    "# avg_income.show()\n",
    "\n",
    "# collect: 클러스터에 분산되어 있는 데이터(RDD나 DataFrame)의 모든 데이터를 드라이버 노드로 가져와서 파이썬 리스트 형태로 반환\n",
    "# for data in avg_income.collect():\n",
    "    # print(data)\n",
    "    # print(data.name)\n",
    "\n",
    "# use function\n",
    "schema_income.groupBy(\"country\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d41d447e-8035-4eec-b134-6da0d7f0691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 16:28:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, col, round as r\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sql_import_csv\").getOrCreate()\n",
    "csv_file_path = \"file:///Users/jeongmieun/study/spark/sample/age.csv\"\n",
    "\n",
    "# header option: either csv has header or note(default: header = false)\n",
    "# inferSchema: either all columns are str or not(guess type)\n",
    "data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(csv_file_path)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43cd09a9-5ab8-4880-ae87-7bb965956e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|             country|avg_age|\n",
      "+--------------------+-------+\n",
      "|                Chad|  36.25|\n",
      "|            Paraguay|  47.78|\n",
      "|            Anguilla|   72.0|\n",
      "|               Macao|   72.0|\n",
      "|Heard Island and ...|   30.0|\n",
      "|             Senegal|   53.0|\n",
      "|              Sweden|  45.33|\n",
      "|             Tokelau|  34.17|\n",
      "|French Southern T...|  50.67|\n",
      "|            Kiribati|  48.67|\n",
      "|   Republic of Korea|  58.17|\n",
      "|              Guyana|   39.0|\n",
      "|             Eritrea|  39.75|\n",
      "|              Jersey|   58.8|\n",
      "|         Philippines|  48.33|\n",
      "|            Djibouti|   38.6|\n",
      "|               Tonga|   49.0|\n",
      "|      Norfolk Island|  35.33|\n",
      "|            Malaysia|  60.67|\n",
      "|           Singapore|   40.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show column name with data\n",
    "# data.select(\"name\",\"age\").show()\n",
    "\n",
    "# filter the data for age of 20 above\n",
    "# data.filter(data.age>20).show()\n",
    "\n",
    "# group by age and aggregates for count\n",
    "# data.groupBy(\"age\").count().show()\n",
    "\n",
    "# custom arithmetic\n",
    "# data.select(data.name, data.age, data.age-10).show()\n",
    "\n",
    "# column alias\n",
    "# data.select(data.name, col(\"age\").alias(\"new_age\")).show()\n",
    "\n",
    "# average\n",
    "# data.select(data.name, data.age, data.country).groupBy(\"country\").avg(\"age\").show()\n",
    "\n",
    "# average and sort\n",
    "# data.select(data.name, data.age, data.country).groupBy(\"country\").avg(\"age\").sort(\"avg(age)\").show()\n",
    "\n",
    "# average and round\n",
    "# agg: groupBy로 그룹을 나눈 후 각 그룹에 대해 집계(aggregation) 연산을 수행하기 위해 사용\n",
    "data.select(data.name, data.age, data.country).groupBy(\"country\").agg(r(avg(\"age\"),2).alias(\"avg_age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec920935-c9af-48c8-8b7f-2b26820b95be",
   "metadata": {},
   "source": [
    "## functions.explode(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51cf85ac-fbf6-420e-be02-9b0a2a1aa0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 16:28:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(anInt=1), Row(anInt=2), Row(anInt=3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "\n",
    "spark = SparkSession.builder.appName(\"df_wordcount\").getOrCreate()\n",
    "\n",
    "# functions.explode(col)\n",
    "# Returns a new row for each element in the given array or map\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, intlist=[1,2,3],\n",
    "        mapfield={\"a\":\"b\"}\n",
    "       )])\n",
    "df.select(functions.explode(df.intlist).alias(\"anInt\")).collect()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19874761-95b1-455d-9fe1-662622c6adfd",
   "metadata": {},
   "source": [
    "## functions.split(str, pattern, limit=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb165db1-5326-4826-b914-d5887e4d49dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word=['hello', 'world', 'and', 'pyspark'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions.split(str, pattern, limit=-1) limit된 숫자의 크기로 나눔\n",
    "# Splits str around matches of the given pattern\n",
    "df = spark.createDataFrame([\n",
    "    Row(word=\"hello world and pyspark\")])\n",
    "df.select(functions.split(df.word, ' ').alias(\"word\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95fd353d-b27a-438f-8eb2-3afa4ac77367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Lorem ipsum dolor...|\n",
      "|                    |\n",
      "|Orci eu lobortis ...|\n",
      "|                    |\n",
      "|Vulputate enim nu...|\n",
      "|                    |\n",
      "|Sit amet nulla fa...|\n",
      "|                    |\n",
      "|Nibh cras pulvina...|\n",
      "|                    |\n",
      "|Arcu felis bibend...|\n",
      "|                    |\n",
      "|Vestibulum sed ar...|\n",
      "|                    |\n",
      "|Sit amet tellus c...|\n",
      "|                    |\n",
      "|Augue mauris augu...|\n",
      "|                    |\n",
      "|Pellentesque mass...|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = \"file:///Users/jeongmieun/study/spark/sample/lorem_ipsum.txt\"\n",
    "\n",
    "# read.text로 가져오면 한 개의 value을 가진 DataFrame으로 저장\n",
    "df = spark.read.text(txt_file_path)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d673ab4-7c4e-4353-8a4c-c5d264b06f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       word|\n",
      "+-----------+\n",
      "|      Lorem|\n",
      "|      ipsum|\n",
      "|      dolor|\n",
      "|        sit|\n",
      "|      amet,|\n",
      "|consectetur|\n",
      "| adipiscing|\n",
      "|      elit,|\n",
      "|        sed|\n",
      "|         do|\n",
      "|    eiusmod|\n",
      "|     tempor|\n",
      "| incididunt|\n",
      "|         ut|\n",
      "|     labore|\n",
      "|         et|\n",
      "|     dolore|\n",
      "|      magna|\n",
      "|    aliqua.|\n",
      "|         Et|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = df.select(\n",
    "    functions.explode(\n",
    "        functions.split(df.value, ' ')).alias(\"word\"))\n",
    "\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ff31a9-1b79-453c-83e9-b9f3b80c0981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|         sed|  194|\n",
      "|          in|  164|\n",
      "|        amet|  149|\n",
      "|         sit|  147|\n",
      "|          ut|  140|\n",
      "|        eget|  131|\n",
      "|          id|  127|\n",
      "|          at|  120|\n",
      "|       vitae|  118|\n",
      "|          et|  117|\n",
      "|        nunc|  113|\n",
      "|          eu|  108|\n",
      "|         non|  102|\n",
      "|            |   99|\n",
      "|          ac|   97|\n",
      "|      tellus|   97|\n",
      "|        diam|   95|\n",
      "|     viverra|   95|\n",
      "|        enim|   93|\n",
      "|pellentesque|   93|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_counts = words.groupBy(\"word\").count().orderBy(functions.col(\"count\").desc())\n",
    "word_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4c457-ca32-4dc1-9005-cb59548a8f57",
   "metadata": {},
   "source": [
    "## types.StructField(name, dataType, nullable = True, metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d14c8b5-6b40-4f07-9458-140c4219b2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- observed_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 16:28:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import(\n",
    "functions as f, \n",
    "Row,\n",
    "SparkSession, \n",
    "types as t\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"df_struct\").getOrCreate()\n",
    "\n",
    "# csv 파일에 header가 없을 때 hearder를 먼저 생성\n",
    "# types.StructField(name, dataType, nullable = True, metadata=None)\n",
    "table_schema = t.StructType([\n",
    "    t.StructField(\"country\", t.StringType(), True),\n",
    "    t.StructField(\"temperature\", t. FloatType(), True),\n",
    "    t.StructField(\"observed_date\", t.StringType(), True)])\n",
    "\n",
    "csv_file_path = \"file:///Users/jeongmieun/study/spark/sample/temp_with_date.csv\"\n",
    "df = spark.read.schema(table_schema).csv(csv_file_path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a841142-58ee-4f32-827a-3d0d0a5da0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|             country|min(temperature)|\n",
      "+--------------------+----------------+\n",
      "|                Chad|           -24.0|\n",
      "|            Anguilla|           -40.0|\n",
      "|            Paraguay|            30.0|\n",
      "|               Macao|           -34.0|\n",
      "|Heard Island and ...|           -39.0|\n",
      "|               Yemen|           -33.0|\n",
      "|             Senegal|           -21.0|\n",
      "|              Sweden|           -29.0|\n",
      "|             Tokelau|           -35.0|\n",
      "|            Kiribati|           -26.0|\n",
      "|French Southern T...|           -22.0|\n",
      "|   Republic of Korea|           -18.0|\n",
      "|              Guyana|           -28.0|\n",
      "|             Eritrea|           -40.0|\n",
      "|         Philippines|           -34.0|\n",
      "|              Jersey|           -21.0|\n",
      "|      Norfolk Island|           -28.0|\n",
      "|               Tonga|           -40.0|\n",
      "|           Singapore|           -25.0|\n",
      "|            Malaysia|           -21.0|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = df.select(\"country\", \"temperature\", \"observed_date\")\n",
    "min_temperature = data.groupBy(\"country\").min(\"temperature\")\n",
    "\n",
    "min_temperature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b3972e1-40a2-42eb-a493-d52819455ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|             country|temperature|\n",
      "+--------------------+-----------+\n",
      "|                Guam|      -13.0|\n",
      "|                Guam|      102.0|\n",
      "|              Serbia|      -31.0|\n",
      "|       French Guiana|       21.0|\n",
      "|Falkland Islands ...|      -40.0|\n",
      "|              Brazil|       59.0|\n",
      "|             Tunisia|      -24.0|\n",
      "|            Portugal|       45.0|\n",
      "|                Iran|       -8.0|\n",
      "|           Australia|       23.0|\n",
      "|              Gambia|       70.0|\n",
      "|               Italy|       88.0|\n",
      "|          Guadeloupe|      -38.0|\n",
      "|        South Africa|      -11.0|\n",
      "|              Malawi|       21.0|\n",
      "|                Iran|       93.0|\n",
      "|      Norfolk Island|       23.0|\n",
      "|Lao People's Demo...|       77.0|\n",
      "|   Republic of Korea|        0.0|\n",
      "|           Singapore|        5.0|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# celsius to fahrenheit: (0°C × 9/5) + 32 \n",
    "f_temp = data.withColumn(\n",
    "    \"temperature\",\n",
    "    functions.round(f.col(\"temperature\") * 9 / 5) + 32).select(\"country\", \"temperature\")\n",
    "\n",
    "f_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c42fd3-a949-41ea-9b61-41d4cd717f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+\n",
      "|    customer_name|cost|\n",
      "+-----------------+----+\n",
      "|     Damion Wolfe|1397|\n",
      "| Benedict Frazier| 998|\n",
      "|  Giuseppe Miller| 997|\n",
      "|    Garret Martin| 997|\n",
      "|Erminia Robertson| 997|\n",
      "|     Milan Gibson| 996|\n",
      "|     Rudy Wheeler| 994|\n",
      "|   Kathey Baldwin| 994|\n",
      "|   Williemae Bell| 992|\n",
      "|Gearldine Aguilar| 988|\n",
      "|      Jewel Parks| 987|\n",
      "|     Hyman Castro| 985|\n",
      "|    Noriko Medina| 984|\n",
      "|     Garfield Day| 982|\n",
      "|      Dacia Adams| 981|\n",
      "|     Taisha Henry| 980|\n",
      "|    Branda Valdez| 978|\n",
      "|     Fumiko Weber| 976|\n",
      "|Geraldo Alexander| 975|\n",
      "|      Walker Pope| 975|\n",
      "+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import (\n",
    "    functions as f,\n",
    "    SparkSession,\n",
    "    types as t\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"df_total\").getOrCreate()\n",
    "table_schema = t.StructType([\n",
    "    t.StructField(\"customer_name\", t.StringType(), True),\n",
    "    t.StructField(\"product_id\", t.IntegerType(), True),\n",
    "    t.StructField(\"price\", t.IntegerType(), True)])\n",
    "\n",
    "csv_file_path = \"file:///Users/jeongmieun/study/spark/sample/product.csv\"\n",
    "\n",
    "df = spark.read.schema(table_schema).csv(csv_file_path)\n",
    "customer_spent = df.groupBy(\"customer_name\").agg(f.round(f.sum(\"price\"),2).alias(\"cost\"))\n",
    "# customer_spent.show()\n",
    "\n",
    "sorted_customer_spent = customer_spent.orderBy(f.col(\"cost\").desc())\n",
    "sorted_customer_spent.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb83665-4cef-4799-9e2f-6396de0b87f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
